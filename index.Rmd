---
title: "Prediction Assignment Writeup"
author: "David Riphagen"
date: "April 23, 2016"
output: html_document
---
# Executive summary
This writeup predicts the manner in which subjects performed and exercise. The model I created is based on a random forest and has an expected out of sample error of ~0.2% (1-accuracy). I also considered a decision tree (accuracy ~49% or similar to coin-flip), boosted trees (accuracy ~0.99%) and lda (accuracy ~64%) models and cross-validated all 4 models on a subset of the training data. Finally, I generated the file "predictions.txt" to predict the 20 different test cases.

# Download and load data
```{r echo = FALSE}
setwd("~/Documents/rclass/ML/Project/practicalmachinelearning/practicalmachinelearning")
```

Download training data
```{r, cache=TRUE, eval = FALSE}
download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", destfile ="pml-training.csv", method = "curl")
```

Load training data
```{r, cache=TRUE}
training <- read.csv("pml-training.csv")
```

Download test data
```{r, cache=TRUE, eval = FALSE}
download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", destfile ="pml-testing.csv", method = "curl")
```

Load test data
```{r, cache=TRUE}
testing <- read.csv("pml-testing.csv")
```

More information on the data collection effort can be found here: http://groupware.les.inf.puc-rio.br/har

# Exploratory analysis of training data
```{r, echo = FALSE}
str(training)
summary(training)
```

There are four sensors:

1. Arm sensor
2. Belt sensor
3. Forearm sensor -> how is this variable different from arm sensor?
4. Dumbbell sensor

More information about the data can be found in the presentation here: http://groupware.les.inf.puc-rio.br/public/2012.SBIA.Ugulino.WearableComputing-Presentation.pdf

# Cleaning up data
First, I found the variables with more than 95% N/As in the training set and store them in the variables na_observations: 

```{r}
na_variables <- sapply(training, function(x) mean(is.na(x))) > 0.95
```

Note that this is a logical vector with 160 observations, for each variable one.
```{r}
str(na_variables)
```

Then I remove these variables from both the training and the test set.
```{r}
training <- training[, na_variables==FALSE]
testing  <- testing[, na_variables==FALSE]
```

This reduces the number of variables to 93.
```{r}
dim(training)
```

Next I remove near zero values with the nearZeroVar function in the caret package.
```{r, echo = FALSE}
library(caret)
nearzerovalues <- nearZeroVar(training)
```

I find 34 variables with values near zero.
Next I remove these variables from the training and testing data:
```{r}
training <- training[, -nearzerovalues]
testing  <- testing[, -nearzerovalues]
```
This reduces the number of variables to 59.
```{r}
dim(training)
```
Finally, I remove identification variables from the training set as these do not relate to the classe variable I would like to predict. The identification variables are in the first five columns ad include things like user names and timestamps.
```{r}
training <- training[, -(1:5)]
testing  <- testing[, -(1:5)]
```
This leaves 54 variables in the dataset:
```{r}
dim(training)
```
# Selecting features
## Correlation
I first take a look at the types of variables in the final training data set: the variables are either numeric or integer, except for the 'classe' variable I am trying to predict.
```{r, echo = FALSE}
lapply(training, class)
```
Next I look at the full correlation matrix for independent variables to see if there are any  that correlate highly with each other.
```{r}
cordata <- cor(training[,-54])
library(corrplot)
corrplot(cordata, order="hclust", addrect=2)
```

When I created a correlation matrix based on hierarchical clustering, there are some variables that seem highly (>0.8, dark-blue and dark-red clusters) correlated. It seems that measurements from the same type of instrument are highly correlated and that also totals are included, some examples:

- Gyroscope metrics are highly correlated, whether measured from dumbbell or forearm
- Acceleration is highly correlated with total acceleration
- Measurements from the magnet arm are highly correlated in any direction (y-axix or z-axis)

## Cross-validation
```{r}
set.seed(1234)
inTrain <- createDataPartition(y=training$classe,
                               p=0.7, 
                               list=FALSE)
training_1 <- training[inTrain,]
training_2 <- training[-inTrain,]
dim(training_1); dim(training_2)
```

# Training
I want to predict the 'classe' variable in the training set. The variables 'classe' has 5 outcomes:

- A: Sitting 
- B: Sitting down 
- C: Standing 
- D: Standing Up 
- E: Walking
```{r}
unique(training$classe)
```
This means I need to predict a classification as outcome, so predicting with (boosted) trees, random forest (set of trees) or linear discriminant analysis seems appropriate.

## Modeling
Modeling with trees
```{r}
set.seed(12345)
modFitDecTree <- train(classe ~ ., method = 'rpart', data = training_1)

library(rattle)
fancyRpartPlot(modFitDecTree$finalModel)
```

Modeling with random forests
```{r, cache = TRUE}
modFit_rf <- train(classe ~ ., method = 'rf', data = training_1)
```

Modeling with Boosted trees
```{r, cache = TRUE} 
modFit_gbm <- train(classe ~ .,
                    method="gbm",
                    data=training_1)
```                    

Modeling with linear discriminant analysis ("lda") model
```{r}
modFit_lda <- train(classe ~ .,
                    method="lda",
                    data=training)
```
# Testing
## Cross-validation
Decision tree
```{r}
predictDecTree <- predict(modFitDecTree, newdata=training_2)
confMatDecTree <- confusionMatrix(predictDecTree, training_2$classe)
confMatDecTree
```
Accuracy is
```{r}
confMatDecTree$overall['Accuracy']
```

Boosted trees
```{r}
gbm_predictions <- predict(modFit_gbm, training_2)
confusionMatrix(gbm_predictions, training_2$classe)
```
Accuracy is
```{r}
confusionMatrix(gbm_predictions, training_2$classe)$overall['Accuracy']
```

Random forest
```{r}
rf_predictions <- predict(modFit_rf, training_2)
confusionMatrix(rf_predictions, training_2$classe)
```
Accuracy is
```{r}
confusionMatrix(rf_predictions, training_2$classe)$overall['Accuracy']
```

Linear Discriminant Analysis
```{r}
lda_predictions <- predict(modFit_lda, training_2)
confusionMatrix(lda_predictions, training_2$classe)
```
Accuracy is
```{r}
confusionMatrix(lda_predictions, training_2$classe)$overall['Accuracy']
```

## Predicting the testing set
The top performing models are gbm (accuracy: 0.9874257) and random forests (accuracy: 0.9979609). Therefore I decide to use the random forest model to predict the testing set.
```{r}
overall_predictions <- predict(modFit_rf, testing)
```

And I write the data to a file:
```{r}
write.table(overall_predictions,
            "predictions.txt", 
            sep="\t",
            quote=FALSE,
            row.names=c("problem_id_1", "problem_id_2", "problem_id_3", "problem_id_4", "problem_id_5", "problem_id_6", "problem_id_7", "problem_id_8", "problem_id_9", "problem_id_10", "problem_id_11", "problem_id_12", "problem_id_13", "problem_id_14", "problem_id_15", "problem_id_16", "problem_id_17", "problem_id_18", "problem_id_19", "problem_id_20"),
            col.names=c("predicted_class")
            )
```

